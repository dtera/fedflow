---
use_lora: true
peft_lora_config:
  r: 16
  lora_alpha: 32
  lora_dropout: 0
  target_modules:
    - "query_dense"
    - "key_value_dense"
    - "dense"
  layers_to_transform:
    - 15
    - 31
learning_rate: 0.005
model_type: "baseline"
pretrained_model_name_or_path: "../../../data/wxg-welm-3b/WELM-3B-32K-Chat"
num_train_epochs: 1
train_dataset_name_or_paths:
  super_glue-boolq: "@tjasad/Slovene_SuperGLUE_BoolQ"
per_device_eval_batch_size: 32
eval_tasks: "boolq"
evaluation_strategy: "no"
eval_steps: 2
do_init_eval: true
do_train: true
do_eval: false
overwrite_output_dir: true
bf16: true
report_to: "tensorboard"
save_strategy: "no"
gradient_checkpointing: false
per_device_train_batch_size: 16
gradient_accumulation_steps: 16
source_max_len: 900
logging_steps: 1
disable_tqdm: false
remove_unused_columns: false
train_on_source: false
warmup_steps: 20
output_dir: "../../../output"
use_cpu: true
