---
use_lora: true
peft_lora_config:
  r: 16
  lora_alpha: 32
  lora_dropout: 0
  target_modules:
    - "q_proj"
    - "k_proj"
    - "v_proj"
  layers_to_transform: "range(0, 24)"
learning_rate: 0.005
model_type: "baseline"
pretrained_model_name_or_path: "Qwen/Qwen2.5-0.5B-Instruct"
num_train_epochs: 1
train_dataset_name_or_paths:
  super_glue-boolq: "@tjasad/Slovene_SuperGLUE_BoolQ"
data_collator: "qlora"
per_device_eval_batch_size: 32
eval_tasks: "boolq"
evaluation_strategy: "no"
eval_steps: 2
do_init_eval: true
do_train: true
do_eval: false
overwrite_output_dir: true
bf16: true
report_to: "tensorboard"
save_strategy: "no"
gradient_checkpointing: false
per_device_train_batch_size: 16
gradient_accumulation_steps: 16
source_max_len: 900
logging_steps: 1
disable_tqdm: false
remove_unused_columns: false
train_on_source: false
warmup_steps: 20
output_dir: "../../../output"
use_cpu: true
